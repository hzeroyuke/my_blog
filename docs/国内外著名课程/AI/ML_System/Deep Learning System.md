来自CMU的课程，Deep Learning System，选择其中一部分课程听了，然后完成Labs

更进一步可以尝试完成LLM System的Lab，以及336上的GPU Part的Labs
## 1. ML Basic

所有的ML系统的训练中，核心过程都是求出Loss，反向传播，Loss Function是一个输入矩阵输出一个标量的函数

而反向传播的过程，是对一个函数求偏导的过程，偏导的结果也是一个矩阵

![[Pasted image 20251003213425.png]]

以上是计算梯度的过程

在反向传播的时候，就是把参数矩阵去减去这个计算出来的梯度乘以学习率

但在实际操作的时候，我们会采用SGD方案，也就是随机的梯度下降，它和标准的梯度下降的区别在于，其不会对整个训练数据集计算梯度，而是随机选择一个minibatch来进行计算，并用这个batch中的样本的梯度的平均值来进行梯度下降

![[Pasted image 20251003214729.png]]

上述过程中我们发现一个现象是，我们的Loss是基于一个Batch的结果，因此我们在求梯度的时候，实际也是在基于一个矩阵计算梯度

