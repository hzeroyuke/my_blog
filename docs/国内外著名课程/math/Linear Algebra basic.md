
听3B1B做的笔记，比较粗糙，为了应对分流面试速成的（虽然最后也没哟个上）
## 1. vector

有两种方案来理解vector

- 方向和长度的集合（起点无关）
- 有序数列 lists of numbers

数学代表着最大的抽象，而数学上的定义是，向量可以保证两个向量相加（向量加法）和数字和向量相乘（向量数乘）是有意义即可

而如何把箭头和有序数列结合起来，我们就需要引入坐标系

![image.png](image.png)

对于数乘，我们引入标量

## 2. 线性空间

在讲解我们的vector的时候，我们引入了坐标系这个概念，其实坐标系某种程度上就暗示了线性空间这个概念

![image.png](image%201.png)

对于坐标系，我们有两个单位向量，坐标系中的所有的向量都可以由这两个单位向量组成，在平面直角坐标系中，这两个向量被称为是basis vector基向量

很显然的，对于平面直角坐标系而言，并非只有上图中的两个向量可以组成basis vector。事实上而言，在平面直角坐标系中任选两个向量，只要他们不共边，就能够组成basis vector

每当我们用list of numbers来表示一个vector时，我们永远和选择的basis vector相关

![image.png](image%202.png)

> span of these vectors 表示这些vectors所能构成的线性组合linear combinations
> 

根据上述定义我们可以推导出，由i和j构成的线性空间，组成了平面直角坐标系

> 我们对于单个向量往往将其看成箭头，对于多个向量往往将其看成一堆点
> 

随后我们可以扩展维度的概念，跳出二维

![image.png](image%203.png)

在三维中，显然两个向量的线性组合无法覆盖空间中的所有向量，当我们选择第三个向量中，如果第三个向量落到了前两个向量组成的面上，那么这三个向量无法构成这个空间的basis 

但是如果第三个向量没有落到这个面上，那么这三个向量则会构成这个空间的basis

从我们的直觉上来看，我们需要的时每个向量都有用，而不是某个向量可以被其他向量表示出来，那么因此我们有如下定义

> Linearly dependent: 表示某个向量a可以被一组向量线性组合
> 

![image.png](image%204.png)

对于一个线性空间而言，如果其中的一组向量线性无关，并且可以张成这个空间，即为其basis vector

## 3. 矩阵和线性变换

Linear transformation，其实就是对于vector的特殊的function，其有以下限制

- Lines remain lineas
- Origin remains fixed

Linear transformation的性质，保证了一个向量相对于basis vector的结果是不变的，比如x = 2a+3b，经过线性变化之后，我们只需要知道线性变化之后的a和b的值，即可知道x的新值

对于一个向量做线性变换，我们只需要关注basis vector的变化即可

![image.png](image%205.png)

这也是为什么我们对于二维向量的变换时一个2x2矩阵，这个矩阵就蕴含了对于原本的二维向量的basis vector做了什么样的变换

![image.png](image%206.png)

比较特殊的情况是，假设的新的“basis vector”线性相关了，则会将整个空间的向量压缩到一个降维度的空间中

![image.png](image%207.png)

## 4. 矩阵乘法和线性变换复合

之前提到矩阵的意义就是线性变换，有的时候我们可以把一个线性变换拆成两个过程，比如一个旋转+一个剪切，这种变换就表现为两个矩阵的乘法

![image.png](image%208.png)

矩阵的正确理解其实应该从右向左计算，先计算右边的线性变换，再计算左边的线性变换

![image.png](image%209.png)

两种变换的叠加顺序是存在区别的，因此矩阵乘法不满足交换律

## 5. 行列式

行列式衡量一次线性变换之后，单位空间的拉伸情况，比如下面这个情况，单位空间就拉伸了一倍

![image.png](image%2010.png)

在这个情况下拉伸了6倍

![image.png](image%2011.png)

当你发现行列式的数值为0时，说明他把所有的点都压缩到了一条线or点上

这个性质非常重要，如果行列式为0，说明他把线性空间压缩到了更小的维度上，比如二维变成一维的线，甚至是0维的点

通过上述的数值计算，我们很自然会发现有负数的行列式，其实是空间定向发生了变化，比如一张纸的两面进行了翻转，形成了所谓负数的面积，其实你可以通过i和j的相对位置来判断这一点

![image.png](image%2012.png)

对于三维线性空间，很显然的应该用体积变换表示行列式的概念

## 6. 逆矩阵，列空间和零空间

**逆矩阵**

![image.png](image%2013.png)

线性代数可以用来解决线性方程组，线性方程组的含义其实是，找到一组向量，使其在进行线性变换之后，和目标向量重合

对于行列式不为0的线性方程组，我们总能找到唯一的解，使其线性变换后于目标向量重合，因为其并未进行降维

而当我们求解的时候，实际是在求系数矩阵的逆向操作

![image.png](image%2014.png)

所谓的逆向矩阵的直观结果，就是经过一个线性变换和逆向线性变换之后，线性空间保持不变，也即 $A^{-1}A=$ 单位矩阵

但是当行列式为0的时候，空间被降维了，不存在对应的逆矩阵可以升维，直观理解上，矩阵=线性变换=函数，函数式一一对应的，但是升维行为是要一对多的结果

![image.png](image%2015.png)

但是值得注意的是，不存在逆向矩阵不等于线性方程组不存在解，比如你降维之后，对应的向量恰好在那条线上

**秩**

我们在降维这个过程会发现，对于三维线性空间，我可以降到二维，也可以一维和0维

我们在线性代数中用rank来表示衡量这个数值，rank是对应矩阵的，表示线性变换之后的空间的维数

![image.png](image%2016.png)

如果行列式不为0，那么其rank等于矩阵的维度

**列空间**

我们将矩阵变换完成之后的空间叫做列空间，因为矩阵的列表示了对于基向量的变换，而新的基向量表示了整个线性空间

rank是列空间的维度

如果rank和列数相同，称为full rank

**零空间**

是指那些经过线性变换之后进入原点的向量

![image.png](image%2017.png)

如果是满秩的线性变换，就只有原点是零空间，如果是降维的线性变换，那就有一系列的向量构成零空间

**非方矩阵**

非方矩阵的含义是我们如果非要做升维or降维操作的结果

![image.png](image%2018.png)

但是实际上没有进行了升维，虽然我们将视角转换成了三维空间，但是其仍然知识三维空间的一个平面

## 7. 点积与对偶性

![image.png](image%2019.png)

点积就是将两个向量逐个元素相乘，并且加起来。也可以理解为a向量在b向量上的投影，再和b向量乘起来

很显然，点积是顺序无关的

对偶性表示的是，两个向量相乘，应当把其中一个向量转成1xN的横矩阵，视为一个线性变换

## 8. 叉积

![image.png](image%2020.png)

叉乘表示为v，w两个向量构成的平行四边形的面积

其是存在方向的v在w的左侧，表示负向的结果，实际上的结果是，我们将v作为第一列，w作为第二列，然后计算这个矩阵的行列式

![image.png](image%2021.png)

很直观的，其相当于把i和j两个单位基向量表示的单位空间，进行线性变换的结果

但是实际上，在叉乘的实际操作中，是两个三维向量乘起来，形成一个新的三维向量

![image.png](image%2022.png)

## 9. 基变换

逆矩阵的概念，就是逆向的线性变换

![image.png](image%2023.png)

M表示真正的变换，$A^{-1}$ 和 $A$ 表示视角的变换bian'hu

## 10. 特征向量和特征值

![image.png](image%2024.png)

在部分线性变换中，有些向量没有变换方向，只是变换了长度

也就是意味着这些向量保留在了张成的空间中，和他们同方向的所有向量，都是如此

这些向量就是所谓的矩阵的特征向量，特征值，就是这一系列向量在这个操作中被拉伸了几倍

![image.png](image%2025.png)

这相当于为矩阵引入了另一种几何视角，我们不用再关注坐标系，而是注重于特征向量的变化

比如说三维变换，如果特征值为1的话，可以看作以特征向量为轴进行旋转

![image.png](image%2026.png)

求解特征向量要如此操作上图中v是一个特征向量