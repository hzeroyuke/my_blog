# 期末复习

* 几种特别的二叉树的定义
* NP 中的formal language 和 VC问题
* Approximation 中的 k-center 问题
* HW 11 题2-3
* HW 12 都不太会
* Random 里面的 quick Sort，好吧都不太会
* quick sort 是 分治
* random sampling
* 最大割问题
* **编程题 + 填空题**
* 小角龙和wwj的Ads笔记
* 背诵分治公式
* skip list? 随机算法，复杂度logn，最坏复杂度为n
* 并行的merge
* ​![8554d4791b0fce8a289ee337b4cf00e](assets/8554d4791b0fce8a289ee337b4cf00e-20240623190822-n294t7c.jpg)​

## 需要记忆的

* AVL 树树高和最小节点数的对应关系
* BP 树的定义
* 摊还的三种方式
* 4 queen的问题有两个解决
* 两个树/堆的摊还分析，势能函数，self-adjusting data structure
* ​![image](assets/image-20240623115842-aaavgg0.png)​
* ‍
* next fit, first fit, best fit, online，offline 的近似率 2 1.7 1.7 5/3 11/9
* ​![image](assets/image-20240623115140-dy2fwt4.png)​
* ​![image](assets/image-20240623115200-nmw4gms.png)​
* ```language
  What makes the time complexity analysis of a backtracking algorithm very difficult is that the number of solutions that do satisfy the restriction is hard to estimate.（True）

  What makes the time complexity analysis of a backtracking algorithm very difficult is that the sizes of solution spaces may vary.（False）
  ```

## 历年卷错题

​![image](assets/image-20240623114510-rdm6g36.png)​

​![image](assets/image-20240622191253-ir3pnvt.png)​

用分治的递推式，每个代入一下就行

​![image](assets/image-20240622191900-fpfom0f.png)​

已解决，画图

​![image](assets/image-20240622194436-hio7674.png)​

我猜是2

​![image](assets/image-20240622194728-0tb2ogj.png)​

每个candidate能大于前k个的概率都是 1 / (k+1)

​![image](assets/image-20240622203856-vz7gvg3.png)​

​![image](assets/image-20240622204432-og1kkw6.png)​

​![image](assets/image-20240622204905-wbdzn6a.png)​

‍

​![image](assets/image-20240622210207-vu8u9yx.png)​

​![image](assets/image-20240623113826-9mr4eny.png)​

​![image](assets/image-20240623120709-xsvvfrq.png)​

​![image](assets/image-20240623121246-xaziryy.png)​

​![image](assets/image-20240623123024-lqldvp4.png)​

答案是D

​![image](assets/image-20240623161656-4gvjqo9.png)​

## Greedy

* quiz 10
* HW 9
* huffman code

  * huffman code 不一定会比正常编码节省时间，有可能生成的树就是完全二叉树
  * 做法：维护使用频率的 min heap 每次取出最小的两个，第一个放左边，第二个放右边；左编码0，右编码1
  * 特性：对于N个字符的字符表，其前缀和最大值不会超过N-1
  * 特性：对于n个字符的字符表，它的huffman tree的node数量为2 * n - 1
* activity select problem
* ​![image](assets/image-20240622105025-jf3vgoc.png)​
* ​![image](assets/image-20240622113950-y3rzb20.png)​
* ​![image](assets/image-20240622120027-z5zfn2x.png)​
* ​![image](assets/image-20240622120706-i7nymfg.png)​
* 关于这道题的第三问可以举一个反例 有 7 5 1 三种面额，总需要兑换17块钱，用greedy 7 7 1 1 1, 最优 7 5 5

‍

## NP

* Quiz 11
* HW 10

* P -> NP(NPC) -> NP-hard -> undecidable problem(例如停机问题)
* NPC 同时是NP也是NP-hard
* NP hard是指可以被所有NP问题多项式规约的问题
* NP 问题 是非确定图灵机多项式解决的问题；也是确定图灵机，多项式时间验证的问题
* ​![image](assets/image-20240622121824-z9i5d1o.png)​
* P = NP 的一种证明方式是证明一个P问题是NPC，例如Shortest-Path
* ​![image](assets/image-20240622124557-r6749bh.png)​
* ​![image](assets/image-20240622124852-wpwkkmd.png)​
* ​![image](assets/image-20240622125306-hows0d6.png)​
* ​![image](assets/image-20240622125333-xmm5v0d.png)​
* ​![image](assets/image-20240622125406-ba0cy19.png)​
* 上题包括了一些常见的NPC问题
* ‍

## Approximation

* Quiz 12
* HW 11 题2-3
* 一些概念

  * An algorithm is called an α(n)-approximation algorithm if it achieves an approximation ratio of α(n)
  * $\alpha = 1 + \varepsilon$ , 所谓$\varepsilon$是近似范式的一个参数
  * 关于近似算法的时间复杂度表示要同时考虑 n 和 $\varepsilon$
  * polynomial-time approximation scheme. 关于n是多项式
  * fully polynomial-time approximation scheme. 关于n和**$1/\varepsilon$**都是多项式
* **bin packing problem**

  * next fit
  * first fit 使用first fit的情况，如果减少一个item，有可能反而变多
  * best fit
  * on-line
  * off-line
  * ​![image](assets/image-20240623113057-gasmc1s.png)​
* **01 backpacking**
* **k-center**
* Approximation algorithm is for finding the near-optimal solutions in polynomial time. 这句话是对的
* ​![image](assets/image-20240622133714-gp9z1h3.png)​
* ​![image](assets/image-20240622134350-bxm65ye.png)​
* ​![image](assets/image-20240622135124-6hesznl.png)​
* ​![image](assets/image-20240622135606-hopqpqn.png)​
* 近似比仅仅是对于一个算法而言
* ​![image](assets/image-20240622135739-5v77a19.png)​
* ​![image](assets/image-20240622140331-qrwej6c.png)​

## local Search

* Quiz 13
* HW 12
* **vertex cover**

  * ​![image](assets/image-20240622141954-jptpreh.png)​
  * original gradiant descent method.
  * Metropolis Algorithm 采用了模拟退火annealing T和K越低，越接近原始的梯度下降算法
* **Hopfield Neural Networks**

  * 迭代的次数最大是所有边权重的绝对值之和
  * ​![image](assets/image-20240622184233-6g5ek8t.png)​
  * 如图，将黑白点化为+1-1 即可计算每个点是否为satisfied，如果小于0，则是
  * 伪多项式时间复杂度，为O（EW）W是边权和
* **maximum cut**

  * ​![image](assets/image-20240622142048-u41kjtl.png)​
  * ​![image](assets/image-20240623153952-0xqde0y.png)​
  * 我们也要考虑降低其时间复杂度
  * ​![image](assets/image-20240623154246-raspxwz.png)​
  * 在此种情况下，它的优化近似比和时间复杂度会变成
  * ​![image](assets/image-20240623154446-g3c5rjb.png)​
  * better local
  * ​![image](assets/image-20240623155651-htqis7n.png)​
* ​![image](assets/image-20240622140956-sirhgj1.png)​
* ​![image](assets/image-20240622141113-6vygxvh.png)​
* ​![image](assets/image-20240622142412-12kg3yi.png)​
* 随机删也可以到达最优解
* ​![image](assets/image-20240622143006-io4pl5x.png)​
* ​![image](assets/image-20240622143047-yh8c3p4.png)​

‍

## Random

* Quiz 14
* HW 13
*  

  * Pr[A] 是A发生概率，$Pr[A]+Pr[\ \overline{A}\ ] = 1$
  * E[X] 是变量X的期望值，其存在下图等式
  * ​![image](assets/image-20240622145742-7g9fk5f.png)​
* **hiring problem**

  * 假设有一家公司要招人，一共招N个候选人，每个候选人面试之后会得到一个分数，我们存在一个约束是我们必须在面试结束后马上确定要不要雇佣他，面试的花费是$C_i$雇佣的花费是$C_h$，面试的花费远小于雇佣，目标是雇佣最优秀的那个人，但是最后可能雇佣了M个人
  * naive solution 和取最大值一样一个个面过来，如果其大于前面的最大值，就雇佣他
  * ​![image](assets/image-20240622150323-tmhkmms.png)​
  * 避免这种最坏情况，我们采用随机化，可以转换为期望的消耗是 $O(C_h*lnN+NC_i)$
  * 如果我们只能雇佣一次，我们采用online 算法，设定k个人都不雇佣，取他们的最高分数，后面的人里面只要有一个高于这个分数就雇佣
  * 这种情形下雇佣第i个员工是最佳员工并且被雇佣到的概率为![image](assets/image-20240622153800-reclov9.png)​
* quickshort 

  * ​![image](assets/image-20240622185709-spmdg3g.png)​
* ​![image](assets/image-20240622143713-2vjnpb6.png)​
* ​![image](assets/image-20240622143924-2mkdl78.png)​
* ​![image](assets/image-20240622144153-ceeq9jy.png)​
* 这题的意思就是m被雇佣，并且他是最好的
* ​![image](assets/image-20240622160329-5et4lgf.png)​
* ​![image](assets/image-20240622160847-bcm8kf5.png)​
* ​![image](assets/image-20240622162050-nylaphl.png)​
* 存在递推式，$a_i$ 是有i个元素的时候的期望次数
* ​![image](assets/image-20240622162252-93cyzhy.png)​

‍

## Parallel

* **一些概念**
* PRAM

  * 多个处理器一个memory的模型，定义了EREW，CREW，CRCW
* WD 

  * In Work-Depth presentation, each time unit consists of a sequence of instructions to be performed concurrently; the sequence of instructions may include **any number**

* EREW 最多只有一个处理器可以读/写
* CREW 多读单一的写
* CRCW 允许多读多写，对于CRCW有几个规则可以选择

  * Arbitary rule 随机选一个
  * Priority rule 选最小number
  * Common rule 只当写同一个value的时候才允许同时写
* **merging**

  * 这里都假设两个数组一样长
  * 基础的rank，对于每个元素并行二分搜索，最后汇总排序：时间复杂度O(logn) 工作量O(nlogn)
  * 正常归并: 都是O(n), O(n)
  * 优化并行，时间复杂度/深度 O(logn) 工作量 O(n) 划分的p是 n/logn
* **summing**

  * 最简单的求和，使用并行，时间复杂度是logn + 2，减少了，w（n）是2n，没变
  * ​![image](assets/image-20240622185308-95f9i6s.png)​
* **prefix sum**

  * 先算B，自底向上
  * 再算C
  * **时间复杂度 O(logn); W(n) = O(n)**
  * ​![image](assets/image-20240608180412-n3tjx8o.png)​
* **maximum finding**

  * 和求和一样建二叉树，算B(h, i) T = O(logn) W = O(n)
  * 使用CRCW，全比较，T=O(1) W=O(n^2)
  * 按$\sqrt{n}$ 分
  * ​![image](assets/image-20240623105502-qh1712k.png)​

    ‍
  * 按照loglogn 分 ![image](assets/image-20240623105414-06fdy1y.png)​
  * random sampling 在 O(1) 的深度和 O(n) 的工作量
* ​![image](assets/image-20240622181055-wf8wxac.png)​
* work load 不一定会下降
* ​![image](assets/image-20240622181356-mcdu3ob.png)​
* ​![image](assets/image-20240622182101-nz4oqqi.png)​
* 可以通过 random sampling 优化

## External sort

* 简单的k way merge: 不包括初始化的run
* ​![image](assets/image-20240623143606-yxznvwj.png)​
