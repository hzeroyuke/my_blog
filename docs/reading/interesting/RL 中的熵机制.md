
来自于青稞社区的一个Talk，介绍了在RL中熵机制的分析以及一些相关工作

https://arxiv.org/abs/2505.22617 相关论文是这一篇

这篇文章出了下述的优化贡献以外，还是一篇简单的综述，有很丰富的信息

熵机制属于是衡量混乱度的一个机制，一个高熵的模型会做更多的探索工作，一个低熵的模型会做更加确定性的回答。RL很依赖于模型的混乱程度，因为模型要做足够的探索，才能得到有效的数据，来优化自身。

LLM在RL中遇到的熵坍缩问题，就是指其陷入了局部最优解，其模型的熵变得很低，不再做有效探索，导致RL失效，Loss不再下降的情况。

在多数模型的训练过程中，性能的提升和Entropy的下降呈现一个兑换关系

![](Pasted%20image%2020251014143547.png)

这个结果导致了我们的RL存在上限，如果不采取一些手段保证模型一定的探索性，会导致模型迅速进入局部最优解

![](Pasted%20image%2020251014144053.png)

那么我们要分析哪些因素导致了我们的熵坍缩，我们发现了一些关键的token，会导致熵急剧下降，这些token表现为其log prob和获得的advantage的cov（协方差）

举个例子，比如这个token的log prob很大，说明模型对于这个token的输出非常自信，同时其获取到的advantage又很大，这就导致这两者的协方差很大，也导致模型很容易因为这个token导致其的熵下降

![](Pasted%20image%2020251014144903.png)

那么要处理这个部分，我们肯定要在Loss上做一些操作，我们选择了两个策略，Clip和KL，核心思路都是对于这个Cov很高的token，对其更新进行限制

