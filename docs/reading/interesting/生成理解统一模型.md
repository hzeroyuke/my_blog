来自于青稞社区的一个Talk，介绍了生成理解统一的模型

https://arxiv.org/abs/2505.14683 相关论文是这一篇 BAGEL

## 1. AR

生成理解统一模型的历史，首先是AR的模型，比如Emu3这种，其就是一个核心的自回归模型，然后把图像这些多模态的元素进行离散化编码（VQVAE，VQGAN），纳入到自回归生成中

![](Pasted%20image%2020251014154052.png)

其主要的难点是在于生成的质量还是比Diffusion Model差，如果需要高质量的生成那么纯AR又会很慢，并且离散化的过程语义信息丢失得也比较多，导致其理解的性能也不是很好

![](Pasted%20image%2020251014154940.png)

对于理解的问题，也有一些解决方案，比如Double Encoder，例如Janus就是采用这种方案，采用VQ-VAE+Siglip双重编码器，能够更好的抽取图像的语义信息，但是相对的开销也会变大，感觉不是一种比较优雅的方案

基于这个现状，Double Encoder显然是一个比较粗糙的解决方案，那么相对应的就由很多想要去做统一的图像Tokenizer，既能够保留底层的纹理信息（VQ-VAE），又可以保留高层的语义信息（Siglip），近期由很多小工作都是这么去做的，比如UniTok，TokenFlow

## 2. External Diffusion

相对于AR方案的另一条路线，就是External Diffusion，外接一个Diffusion Model的方案


## 3. Unified Model

在一个transformer中同时训练自回归语言模型和Diffusion Model，来达到生成理解统一的结果

![](Pasted%20image%2020251014213741.png)


