来自于青稞社区的一个Talk，介绍了生成理解统一的模型

https://arxiv.org/abs/2505.14683 相关论文是这一篇 BAGEL

## 1. AR

生成理解统一模型的历史，首先是AR的模型，比如Emu3这种，其就是一个核心的自回归模型，然后把图像这些多模态的元素进行离散化编码（VQVAE，VQGAN），纳入到自回归生成中

![](Pasted%20image%2020251014154052.png)

其主要的难点是在于生成的质量还是比Diffusion Model差，如果需要高质量的生成那么纯AR又会很慢，并且离散化的过程语义信息丢失得也比较多，导致其理解的性能也不是很好

![](Pasted%20image%2020251014154940.png)

对于理解的问题，也有一些解决方案，比如Double Encoder，例如Janus就是采用这种方案，采用VQ-VAE+Siglip双重编码器，能够更好的抽取图像的语义信息，但是相对的开销也会变大，感觉不是一种比较优雅的方案

基于这个现状，Double Encoder显然是一个比较粗糙的解决方案，那么相对应的就由很多想要去做统一的图像Tokenizer，既能够保留底层的纹理信息（VQ-VAE），又可以保留高层的语义信息（Siglip），近期由很多小工作都是这么去做的，比如UniTok，TokenFlow

## 2. External Diffusion

相对于AR方案的另一条路线，就是External Diffusion，外接一个Diffusion Model的方案，Qwen-Image，Blip-3o这些都是这么做的


## 3. Unified Model

在一个transformer中同时训练自回归语言模型和Diffusion Model，来达到生成理解统一的结果

![](Pasted%20image%2020251014213741.png)

## 4. 理解真的可以指导生成吗

这是近期的一篇文章讲述的内容，叫做 [UniSandbox](https://arxiv.org/abs/2511.20561) ，这是一篇分析性质的文章，分析近期一些生成理解统一模型，是否真的能够提高生成能力

这篇文章主要分析了三类模型

- 纯自回归 Janus-Pro 这类
- AR+Diffusion 不同模型，Qwen-Image和Blip-3o这类
- AR+Diffusion 统一模型，BAGEL

文章发现在没有显式的Cot的情形下，模型在需要理解的生成情况下其实能力堪忧，比如无法正确生成3+2个苹果

![](asset/Pasted%20image%2020251204155709.png)

文章还提出了一种框架，给统一模型引入课程学习，来引导其推理能力的增强，其实是一个比较标准的拒绝采样微调的能力，用cot模型来完成推理，并且用vlm模型来判断这个cot和生成的图像是否对应，获得一批高质量数据，来sft

![](asset/Pasted%20image%2020251204155758.png)

知识迁移能力是这篇文章另外讨论的部分，实验中，将一些编造的知识注入到模型中，看模型会不会采用这些知识来辅助他们的生成，实验表明，在不经过显式Cot的情况下，模型并无法将知识作为辅助生成的一部分。比如我们告诉它A=3，让它生成A个苹果，其无法完成这个任务。

并且发现Query-Based架构，也就是类似Blip-3o的模型，在知识迁移任务，有更加显著的优势



