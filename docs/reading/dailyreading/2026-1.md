
## 1.1

- Video Models are Zero-Shot reasoner and learner
	- Google Veo3 的论文，展示了Veo3在经过大规模训练之后涌现出来的Zero shot的能力，能够完成训练数据以外的各种任务，展现了惊人的涌现能力

## 1.2

- mHC https://arxiv.org/abs/2512.24880
	- DeepSeek的论文，对于残差流进行了修改，将恒定为1的残差流转换成一个可学习的目标，并且将其进行Manifold-Constrained防止残差流变得过大
- Evaluating Parameter Efficient Methods for RLVR
	- 对于RLVR任务中高效参数微调方案的一个分析实验，包括Lora，DoRA、AdaLoRA等变体

## 1.4

- [JavisGPT](https://www.arxiv.org/abs/2512.22905) 
	- 音视频编辑的模型，通过一个QwenVL+Dit拼接的架构，完成了多模态的理解和生成，包括编辑，并且着重优化了音视频协同的内容
	- 输入包括video的输入，音频的输入，文本提示词。基于QwenVL，然后适配了一个Lora来进行微调。最后LLM会将输出给一个Dit，让其输出编辑好的视频音频内容

![](asset/867cc7bd6b4de1c08fdf03f03ecb0935.png)

生成和理解是否需要纳入在统一的框架中，现有的方案似乎仍然没有指出一条明确的路径。从nano-banana的角度来看，似乎目前最好的方案还是让生成理解进行分离，作为一个系统进行处理

- https://arxiv.org/pdf/2512.24873 Agent Learning EcoSystem
	- 是阿里团队做一个完整且系统的Agent学习框架，将阿里之前做的很多Agent相关的工作串了起来，并且致力于解决训推不一致，力图构建足够Strong的Agent RL baseline
	- ROLL：支持异步 rollout-训练解耦、动态 GPU 分时复用、chunk 级信用分配的 RL 训练框架。训练框架
	- ROCK：万级并发、安全隔离、GEM API 兼容的沙箱环境引擎，实现轨迹采集与运行时验证。核心是沙盒
	- iFlow CLI：统一上下文管理、可配置工作流的代理框架，保证“训练-部署”零差异。核心是上下文管理

![](asset/Pasted%20image%2020260104183643.png)

- [Recursive Language Model](https://arxiv.org/pdf/2512.24601v1)
	- 这篇论文主要针对的是超长上下文的问题，其让LLM将上下文作为一个环境变量去处理，而不是每次都一股脑地把所有上下文都塞进模型中
	- 让模型自己编写Python代码来拆解上下文和调用自己，来实现上下文的高效处理

![](asset/Pasted%20image%2020260104190459.png)


## 1.8

- https://nitrogen.minedojo.org/
	- nvidia的工作，收集了超过40000+h游戏视频，覆盖超过1000款游戏，并且提取了动作片段还进行了打标，用这些数据训练一个游戏Agent

![](asset/Pasted%20image%2020260108143332.png)

- [Loza](https://arxiv.org/pdf/2512.23966v1)
	- 美团开发的稀疏注意力机制，基于MLA，将标准的MLA拆成两部分，全量MLA+SSA（Stream Sparse Attention）

![](asset/Pasted%20image%2020260108171417.png)


## 1.9

- [NextFlow](https://arxiv.org/pdf/2601.02204)
	- 用Next Scale Prediction来做生成理解统一模型
- [SemanticGen](https://jianhongbai.github.io/SemanticGen/)
	- 使用两阶段的生成方案，先生成High-Level的语义表征，再在VAE的空间中生成细粒度的纹理结构，当然与此相对的，训练也要训练两部分，Semantic Generator和VAE Latent Generator

![](asset/Pasted%20image%2020260109161146.png)


## 1.12

![](asset/Pasted%20image%2020260112190426.png)

Sida Peng老师整理的2025年关于Video World Model的相关论文，可以借这张图来查漏补缺

- [VideoAuto-R1](https://ivul-kaust.github.io/projects/videoauto-r1/) 
	- 关于Video Understanding做RL的论文，用两阶段的输出方案，首先让其直接输出答案，根据答案的置信度再判断要不要进行Cot
- [MIMO-v2-Flash](https://arxiv.org/pdf/2601.02780)
	- 小米的flash模型的技术报告，在架构上，和很多新的模型一样采用了混合架构，它采用了滑动窗口+Global Attention的混合架构
	- 在滑动窗口中引入了可学习的Attention Sink Bias，在softmax的分母里增加一个可学习的项，在模型不需要关注当前窗口内Token的机会，将注意力分配给虚拟的Sink
	- 滑动窗口大小仅有128 token，在消融实验中效果比512 token好
	- 引入了SD做MTP


## 1.13

- [Plenoptic Video Generation](https://arxiv.org/pdf/2601.05239)
	- 这篇文章做的是多视角的Camera Control，核心贡献是在解决长时记忆的一致性，在不断的镜头变化之后，生成的视频还能保持一致性，比如在视角转一圈之后，回来看到的还是一样的场景
	- 多视角的re-render比单视角的更加困难，多视角一次性渲染多个视角的视频，要求保证这几个视频之间能够保证足够的一致性。该论文解决的任务中，定义为输入video+多个相机轨迹，输出多个相机轨迹对应的video
	- 解决长时记忆的一致性的核心是给Dit的注入条件
		- 最Native的方案，将所有的视频都塞进去，但是无疑是极大的开销，尤其是高分辨率视频的场景下
		- 用Block-Diffusion的方式，放弃一次生成多视角的视频，而是按照自回归的方案一个个生成，在逐个生成的时候，不可避免的上下文也会越来越长，因此就要做检索生成，这篇文章基于3D Fov来做检索生成，上下文中只塞入和当前视角相关的视频上下文。比如我们某个视角要看房子的背面，我们主要关注其他同样会划过房子背面的视角


![](asset/Pasted%20image%2020260113134639.png)


- [Video Deep Research](https://arxiv.org/pdf/2601.06943)
	- 使用Deep Research以及一系列的工具，来做Video Understanding的增强（其实感觉不是很make sense，也就是补足一些世界知识和一些时效性的场景会有效
- [BabyVision](https://arxiv.org/pdf/2601.06521)
	- 做了一个Benchmark来评测VLM的能力和婴幼儿的视觉能力的差异，在这个测评集合中最强大的VLM的能力也不如6岁的儿童

![](asset/Pasted%20image%2020260114135641.png)


## 1.14

- [Engram](https://github.com/deepseek-ai/Engram/tree/main)
	- DeepSeek的新论文，对Transformer架构进行了修改，新增了一种条件记忆机制称为Engram，研究者认为Transformer架构中缺少原生的查找模块，被迫使用计算来模拟检索机制
	- 检索模块采用现代的N-gram模块，将部分MOE替换为Engram内存，可以达到一个权衡，并且在长上下文任务上有很大的提升
	- Engram的模块的功能包括检索+融合两个部分，经过压缩的分词器之后，通过一个Multi-Head Hash，最后拼接成一个记忆向量，后面过一个Context-Aware Gating

![](asset/Pasted%20image%2020260114141740.png)

在相同的参数预算下，Moe参数和Engram所占内存之间存在U形曲线的兑换关系

![](asset/Pasted%20image%2020260114141932.png)

并且在推理过程中，Engram可以做到存算分离，Engram模块的计算和中间层的输出无关，仅仅和输入的token序列有关，也就是说我们可以将Engram模块的计算和Transformer-Block的计算解耦开，并且此种方案可以有效利用CPU内存

![](asset/Pasted%20image%2020260114142654.png)

- [Motion Attribution for Video Generation](https://research.nvidia.com/labs/sil/projects/MOTIVE/)
	- nvidia的论文，训练数据中的视频如何影响视频生成模型中的运动输出，因为Image生成模型已经做得非常优秀，人们往往致力于将Image Model的一些做法迁移过来，包括对于纹理，色彩等内容的优化。但是这些过程往往忽略了视频生成模型最为独特的内容，运动，运动和时间流动紧密联系，是视频生成模型最重要的特点
	- 核心是提供了一种数据筛选的方案，来使得训练的模型有更好的运动一致性

**Data Attribution**

这个是数据处理领域的一个概念，用于研究每条数据对于模型训练带来的影响，这个领域的发展大概有这些阶段

- Ex-DL时代，前DL时代，模型较小，可以用每次更新之后模型对于测试集的变化（这里需要一个Influence Functions）来衡量，这一步需要矩阵求逆，在现代DL模型下时效
- 现代DL方案，会用类似TracIN，TRAK的方法来近似数据影响的计算
- Diffusion Model：对于Diffusion Model的衡量和其他方案有些差异，因为Diffusion不同时间步有不同的表现，因此需要norm

![](asset/Pasted%20image%2020260114162356.png)

而这些方案对于Video diffusion领域都失效了，传统的Diffusion Data Attribution的方法会导致其关注外形而不是动作，并且在Long Video下计算开销也会扩大，这也是这篇论文的核心贡献

![](asset/Pasted%20image%2020260114162950.png)



## 1.15

- [OctoCodingBench](https://huggingface.co/datasets/MiniMaxAI/OctoCodingBench)
	- MiniMax发布的新的关于Coding的Bench，是一个非常详尽的过程监督的Benchmark，会监督模型在解决Coding任务的过程中，有没有遵从用户指示，不止关注最终的测试是否通过

![](asset/Pasted%20image%2020260115144306.png)

- [MemGovern](https://arxiv.org/pdf/2601.06789)
	- 经验库+Code Agent，希望Code Agent在执行编程任务的时候，能够参考一些人类的成功经验来提高成功率，标记了130+k从Github的获取的成功经验作为Code Agent的经验库，但是感觉涨点不是很高
	- 个人认为作为编程辅助的工具的话，如果Code Agent在无法处理任务的时候，能够帮我从Github，StackOverflow上帮我搜索相关的解决方案，并且列出来给我看，这个是一个我会比较喜欢的功能

![](asset/Pasted%20image%2020260115145929.png)