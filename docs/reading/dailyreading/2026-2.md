
## 2.3

- [context learning bench](https://www.clbench.com/)
	- 腾讯发布的一个benchmark，关于测评模型上下文学习的能力，也即在上下文纳入新的知识和新的规则，看模型的遵循程度
	- 最高分模型为GPT 5.1 仍然不到30%的成功率，模型往往会更倾向于使用自己的世界知识


## 2.4

- [Code OCR](https://arxiv.org/pdf/2602.01785)
	- 利用VLM的能力，将代码输入从文本转换成图像，力求进一步压缩提高效率
- https://arxiv.org/pdf/2602.01630
	- 本文是Kling的一篇关于World Model的讨论，其论述了World Model应该包含的内容和功能，并且批判了当前World Model的前进方向，主要是在某个具体的任务，比如vision generation上注入物理规则的知识
	- 本文提出，一个World Model的框架或者模型，必须具有以下内容和功能
		- Interation 多模态的感知+动作接口
		- Reasoning 因果推理的能力
		- Memory 结构化，可压缩，动态更新的长时记忆
		- Environment 可生成，可响应，物理一致的仿真/真实环境
		- Multimodal Genaration 多模态的生成能力

![](asset/Pasted%20image%2020260204184351.png)

- [Paper Banana](https://dwzhu-pku.github.io/PaperBanana/)
	- 利用nano banana构建Agent workflow，用于实现精美的科研制图

![](asset/Pasted%20image%2020260204185615.png)

- [Seed Prover](https://arxiv.org/pdf/2512.17260)
	- Agentic Prover for Math Problem

![](asset/Pasted%20image%2020260204194622.png)


## 2.8

- [residual context diffusion](https://yuezhouhu.github.io/projects/residual-context-diffusion/index.html)
	- 先前的diffusion llm每次解码完成之后，都会将置信度低于阈值的token remask，但是这一部分token同样存在丰富的语义信息，这篇论文将这一部分token reweight之后转换为下一次预测的上下文

![](asset/Pasted%20image%2020260208135258.png)

- [Parallel Coordinated Reasoning](https://arxiv.org/pdf/2601.05593)
	- 并行解码上百条思维链，然后通过总结压缩的方案进行汇总，在不扩展上下文窗口的情况提升探索性能
- [lingbot depth](https://technology.robbyant.com/lingbot-depth)
	- 针对消费级相机在玻璃，镜面和金属表面等高难度场景下进行深度分割，使用的方案是Masked Depth Modeling，也就是将这些高难度场景的部分建立mask，用整个画面中的RGB信息对其进行预测
	- 与DA3这些模型的区别在于，DA3是在一个单图生成的深度估计，而这个项目是补全工业场景的一些深度摄像头，也就是对工业摄像头的一个补强，而DA3是一个通用模型
- [Golden Goose](https://arxiv.org/pdf/2601.22975)
	- 一种将常规的文本转换成RL数据的一个方案，提供了一个数据处理pipeline

![](asset/Pasted%20image%2020260208195332.png)


## 2.10

- [thinkRL-edit](https://echopluto.github.io/ThinkRL-page/)
	- 本质是对于image edit模型进行RL，但是其模型架构中既包含了推理模型又包含了生成模型

![](asset/Pasted%20image%2020260210185952.png)

- [large concept model](https://arxiv.org/pdf/2512.24617)
	- 对于tokenizer的部分做出改动，tokenizer对于语义进行静态的表征，但是语言中部分概念承载了大量的推理信息，另外的部分则较为稀疏
	- 因此在这个论文中，提出在latent space中学习动态的语义边界，讲序列动态分割为概念片段

 ![](asset/Pasted%20image%2020260210190513.png)

## 2.12

- [large lora rl](https://macaron.im/mindlab/research/building-trillion-parameter-reasoning-rl-with-10-gpus)
	- Mind Lab 提出的，针对超大规模参数，比如1TB的kimi模型进行lora rl微调的方案
	- 使用的是verl和megatron bridge
	- 相比于全量微调方案，只需要10%的GPU即可完成微调
- [scale agent rl in terminal environment](https://faithful-almanac-add.notion.site/The-Bitter-Lesson-Behind-Building-Agentic-RL-in-Terminal-Environments-2eaddd45837f80c9ad2ed6a15ef3c1a1)
	- 这是qwen团队的一篇博客，讲述了如何用ROLL, ROCK和iflow来实现稳定的agent rl训练
	- 比较细节地讲述了其中的各项取舍和踩坑

![](asset/Pasted%20image%2020260212142437.png)


## 2.24

- [lingbot world](https://github.com/Robbyant/lingbot-world)
	- 也是block diffusion范式下的视频生成


## 2.27

- Reasoning LLM，这张图比较完整地介绍了Reasoning LLM的发展状态，比如是什么，为什么，怎么做

![](asset/Pasted%20image%2020260227215533.png)



## 2.28

- [GradLoc](https://hy.tencent.com/research/100015?langVersion=zh)
	- 混元提出的一个工具，根据梯度的异常现象，自动定位到触发异常的token，并且分析原因，包括训推不一致，层间梯度异质性

![](asset/Pasted%20image%2020260228155805.png)

- https://arxiv.org/pdf/2601.18217
	- 这篇Meta的论文讲述了在对少量的domain做Post-Train的，如何在general的现实环境中生效，有这样一些Insights
	- 在原始的环境观察中注入少量目标无关的文本信息，在OOD人物上获得了可观的成功率提升
	- Step by Step Thinking 在In-domain任务不一定有性能提升，但是对于OOD任务提升显著
- [LOCA bench](https://arxiv.org/pdf/2602.07962)
	- 超长上下文的agent任务 benchmark
- [Prism block sparse attention](https://efficacious-citrus-7a0.notion.site/Prism-Spectral-Aware-Block-Sparse-Attention-304d97f5df9d80318802f9cb37d18c3e)
	- 基于光谱感知的Block Sparse Attention
	- 

