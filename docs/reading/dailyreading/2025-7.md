# 2025.7

## 7.1

- https://arxiv.org/abs/2506.21545
    - 微软的数据处理的paper，系统论述了如何优化数据的组织的方式来提高语言模型的性能，在数据内容和模型架构不变的情况下如何提高模型性能
    - 与传统的数据打乱和混合方式不同，该工作对data进行了评分，按照这个评分进行筛选和排序再进行训练
    - 并且在训练的过程中，还会进行有选择地重复数据的训练Folding Order来避免模型遗忘等问题
- https://arxiv.org/abs/2506.22419
    - Meta提出的Agent benchmark，其中的task是论文复现，复现GPT2的训练加速工作，这个加速工作来自于一个竞赛，竞赛的每个提交都有对应的训练脚本和说明，测试Agent能否基于上述信息做出有效的复现，对于相关竞赛地评审工作也比较有实际意义
    - 测试使用的Agent框架是Scaffolds extends AIDE ，AIDE是一个exploration agent的框架

> Workflow Agent和Exploration Agent的区别，在看论文的阶段注意到现在Agent的方向逐渐分化出两个目标
- Workflow Agent，是人类知道该怎么做，但是希望借助Agent的能力将类似的工作自动化（Manus，各种MultiAgent）
- Exploration Agent，是人类不知道怎么做，例如解决一些科研学术难题，借助Agent的能力进行探索（AIDE，ML-Master），往往会采用Tree or Greedy的方案进行探索
> 

- https://arxiv.org/abs/2506.16499
    - 上交提出的Agent，在MLE-Bench上刷到了SOTA，MLE-Bench中有75个Kaggle上的真实机器学习任务组成
    - 目标是达到推理和搜索的平衡统一，未进行训练，代码即将开源
    - 搜索树的节点是一个方案的某个状态，树的边是指行动，例如Draft，Debug or Improve
    - 核心是应用了一种自适应的memory机制

## 7.2

- https://fmhy.net/ai
    - 全球各种免费资源汇总链接，包括娱乐资源和开发资源
- https://sierra.ai/
    - 开发客服Agent服务，相比于之前的pine.ai是帮用户去向客服打电话，从而帮助用户避免反复的打各种电话消耗的精力，Sierra.ai的服务就恰好相反，是去自己作为客服去接用户的电话
    - 在Agent沟通这个领域和pine.ai有相同的考量，比如如何保证实时性和充分的思考，同时也有一些差异化的思考，比如用户的电话语音往往会带有背景杂音，因此其在背景音过滤和判断上做了比较多的努力
    - 需要集成主流CRM，CDP，ERP系统
    - 其构建了一个Agent OS来帮助快速搭建需要的Agent system
- ‣
    - LangSmith是LangChain开发的一个组件，用于检测Agent行动的每一步的Context，用来分析每一次LLM call是否都给出了准确的上下文
    - 不仅限于基于LangGraph开发的Agent，是否需要付费？
- https://arxiv.org/abs/2506.24119
    - 一个用自我博弈来代替奖励函数进行迭代的RL方案
- https://arxiv.org/abs/2506.23235
    - 用逆强化学习，将通用大模型转换成通用的Reward Model，也即每个模型可以自己从自己的rollout中计算奖励

## 7.3

- [‍‬⁠﻿⁠‌‍⁠⁠⁠﻿‬‍‬﻿‍‌﻿⁠‬﻿‍‬‌﻿⁠‬⁠Gemini CLI Prompts - 飞书云文档](https://eqhophsxip.feishu.cn/docx/PRJdd3xxZoMeOpxD2dbcC8oAnxb)
    - Gemini CLI 的Prompt设计，如何给case如何做强调

## 7.4

- https://github.com/anthropics/dxt
    - Anthropics官方推出的服务，用于解决MCP的环境冲突问题，可以做到一键安装MCP extension而不需要安装Node，Python等环境

## 7.7

- https://github.com/MemTensor/MemOS
    - 一个较为完备的Memory系统，今天刚刚开源，应该是sota的，甚至在部分场景超越了Full-context的表现，可以借此项目对于Memory，Rag，Graph DB等系统有一个完整的了解
- https://arxiv.org/abs/2507.02592
    - 阿里在WebDancer上继续做的Agent工作，是Deep research agent RL的工作
- https://arxiv.org/abs/2507.02259
    - 字节的Memory Agent，设计了一种Memory机制可以让Agent自主地修改，将Memory机制作为工具，用RL进行训练，可以在长任务中达到很好地效果

## 7.8

- GEO，与SEO相对的概念，设计自己的网站，使其内容布局更容易出现在Gemini的捕获路径上
- https://github.com/zaidmukaddam/scira
    - 开源版的perplexity，结合了很多的搜索源

## 7.9

- [[2507.05169] Critiques of World Models](https://arxiv.org/abs/2507.05169)
    - 对于当前World Model的批判，认为World Model不应该局限于长视频生成领域，应该引入更加丰富的模态，并且提出World Model应该是Agent未来的模拟沙盒
- https://pokee.ai/
    - 现阶段开放了一个workflow agent给我们，根据用户的需求构建Workflow，随后执行
    - 目前看了一下它的工作流还是，拟定计划+收集信息+执行三步走，比较通用的工作流，但是其似乎对提示词比较敏感，同时也经常出现搜索出错卡住的情况
    - 没有发现回退和循环的设计，但是其搜索显示了其是并发搜索

## 7.11

- https://arxiv.org/abs/2507.07017
    - 字节发布的对于RL理论的一些探索，对于现阶段RL在探索阶段不稳定的分析，对于RL的理论可以用来加深
- [[2507.06229] Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)
    - 一篇关于Agent经验总结的论文，如何在不同的Agent中复用之前的经验，其提供了教师-学生代理，让学生代理去学习教师代理之前的经验
    - 其用smolagent作为教师模型进行GAIA上的测评（不过这个在同一个benchmark上积累经验真的不是幽默吗
- [[2507.05791] GTA1: GUI Test-time Scaling Agent](https://www.arxiv.org/abs/2507.05791)
    - 对于GUI agent的RL
- [[2507.05257] Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions](https://arxiv.org/abs/2507.05257)
    - 对于Agent Memory的benchmark
- [[2507.05578] The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation](https://www.arxiv.org/abs/2507.05578)
    - 关于模型Memory机制的一个综述

## 7.14

- https://mirix.io/
    - 这个memory系统本身就是一个multiagent系统，优势在于对于多模态信息的处理能力很强，附带有自动截图的功能，推测和ComputerUse的相性会比较好
- https://www.arxiv.org/abs/2507.07969
    - 基于RL的模仿学习？
- https://www.arxiv.org/abs/2507.07955
    - LLM的No-tokenizer方案，H-net

## 7.15

- [Goodie AI: Answer Engine Optimization & AI Search Visibility Platform](https://www.higoodie.com/)
    - GEO的产品之一，让你的产品能够快速出现了Claude，Deepseek和Gemini等的搜索路径中，和SEO对称，变成了一种投放广告的方案
- [Overview](https://www.mcpauth.app/)
    - MCP auth，是用于给MCP server提供Oauth托管的服务，开源

## 7.16

- [[2507.00432] Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
    - 论证RL对于推理能力的泛化性

## 7.17

- [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/pdf/2507.09477)
    - Rag + Reasoning的综述
- [[2507.10524] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
    - 一篇关于推理加速的论文 同时使用了参数共享+自适应计算+KV cache

## 7.18

- [[2507.11988] Aime: Towards Fully-Autonomous Multi-Agent Framework](https://arxiv.org/abs/2507.11988)
    - 字节的Multi-Agent系统，以灵活性为主动，可以通过Agent工厂构建不同类型的Agent，动态地调整任务和监控任务情况，对抗Multi-Agent系统的不灵活性
    - 在各个benchmark上都能比肩专用Agent

## 7.25

- [[2506.01939] Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
    - 阿里的一篇文章，讲述了在强化学习中，不是每一个token都是同等价值的，比如有一些步骤在预测的时候，有好几个token是相同的概率，这类token被成为High-Entropy的tokens，如果只优化（反向传播）20%的High-Entropy tokens，有概率可以使得全量优化更好的结果

## 7.28

- [[2507.18074] AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
    - 上交开发的Multi-agent系统来做模型架构的提升，自主探索新的模型架构来提升模型能力
- [[2507.18634] Captain Cinema: Towards Short Movie Generation](https://arxiv.org/abs/2507.18634)
    - 字节的一篇关于长视频生成的内容，通过关键帧生成模块+视频生成模块+Memory模块，来实现了短电影的生成能力
    - 实际看起来效果一般，像是一段一段关键帧生成的GIF拼接起来，人物的动作和分镜都不够流畅
- [[2507.18624] Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
    - CMU的一篇文章，讲述一种增强模型指令遵循性的方案，标记数据是人类的指令+一连串关于这个指令yes or no的结果，基于这个方案做RL
    - 个人认为在规划方案中，这个范式没准也make sense，可以用来给用户提供一些默认回答
- [[2507.17746] Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746)
    - 扩展reward function地能力，在医学，法律等方面获取了有效地进展，方案是设计一系列地打分标准，就和高考判语文题目一样，作为verification，进行RL

## 7.29

- [[2507.19427] Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding](https://www.arxiv.org/abs/2507.19427)
    - 阶跃星辰近期发布的新版大模型
- [Chat with Z.ai - Free AI for Presentations, Writing & Coding](https://chat.z.ai/)
    - 智谱近期发布的新版大模型
- [[2507.19457] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://www.arxiv.org/abs/2507.19457)
    - 关于提示词优化的工作，可以根据这个工作学习一下如何优化系统提示词
    - 核心在于多目标优化，Pareto的概念：“**在多个互相冲突的目标上，再也找不到任何“一个目标变好而另一个不变差”的候选方案的集合。**” 使得Prompt不至于变为局部最优
- https://github.com/jd-opensource/joyagent-jdgenie
    - 京东开源的agent，一个较为通用的multi-agent系统