## 12.3

- https://arxiv.org/abs/2512.03036
	- VisAudio：基于视频的双耳音频生成，同时还做了一个数据集，包含了上千个视频+音频的数据集
- https://arxiv.org/abs/2512.02556
	- DeepSeek V3.2：主要介绍了其中的稀疏注意力机制，以及一些在RL中的稳定性机制
	- 稀疏注意力机制：DSA，先走一遍相关度计算，随后选择top-k的key value进行attention计算
	- RL机制：GRPO的改进，无偏KL，Off-policy 序列掩码，Keep-Routing，Keep-Sampling-Mask

## 12.8

- https://appletea233.github.io/think-while-edit/ 
	- edit-thinker：通过一个edit model+MLLM的交织行为，进行强化学习，完成test time scaling，先进行edit，然后再经过MLLM的分析给出调整的指令，继续edit，直到达到标准
	- 以GPT-4.1为教师，训练了一个MLLM，会给出指令调整，作为一个可插拔的组件和各种edit model放在一起，并且提升性能
- https://arxiv.org/abs/2512.04810
	- EMMA：高效的生成理解统一模型，构建了一个更高压缩率的编码器，并且稍微修改了模型架构


![](asset/Pasted%20image%2020251208222525.png)

## 12.10

- https://arxiv.org/abs/2512.07461 
	- Native Parallel Reasoner，并行推理的训练

![](asset/Pasted%20image%2020251210153138.png)

- https://arxiv.org/abs/2512.07469
	- Unified Video Edit model 在video edit中引入了思维链，避免之前的edit工作要先一个模型做mask的方案，将先mask再编辑的方案统一到一个模型中
- https://arxiv.org/abs/2512.07778
	- DMVAE，常规的Vae遵从高斯约束，这篇论文尝试显式地约束Vae压缩地空间，并且测试哪些空间流形对于生成最有效
- https://arxiv.org/abs/2510.27688
- https://shaochenze.github.io/blog/2025/CALM/
	- CALM  这是一篇比较早的工作，是在连续空间内，做语言模型，预测一个连续向量而不是离散的


## 12.12


- https://arxiv.org/pdf/2512.08296
	- Agent Scaling，Google发表的研究，用来分析Multi-Agent System（MAS），在Agent数量改动以及通信机制变更的情况下，会有什么样的性能变化。做了很多有效的消融实验，剥离提示词工程的影响，来真正的分析Multi-Agent system的性能

![](asset/Pasted%20image%2020251212183234.png)

以上是不同的通信策略


![](asset/Pasted%20image%2020251212182841.png)

这是MAS在不同任务上的影响，于单个Agent系统（SAS）相比

## 12.15

- https://zhuanlan.zhihu.com/p/1982065391216923209
	- CLIP到Qwen3-Omni，多模态模型的融合的一份综述
- https://arxiv.org/abs/2512.11749
	- SVG-T2I，Kling的工作，RAE的衍生工作，不止在ImageNet，在高分辨率场景下，做无VAE的生成模型，使用DINO-v3的特征
- https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf
	- OpenAI的工作，极致稀疏的神经网络，可能是可解释性的突破口，全连接层的特性导致其的黑盒状态

## 12.16

看到一篇帖子，是对nips 2025做的架构改进的总结

- Attention
	- [Gated Attention](https://arxiv.org/pdf/2505.06708) Qwen的工作，在attention计算中增加一个门控模块，因为每次softmax计算的加权和都为1，但是并不是每次的Q都和Key都有足够的联系，因此有时候attention计算引入了不少噪音
	- 核心操作在于在Attention计算之后，利用gate score进行加权

![](asset/Pasted%20image%2020251216164724.png)

```python

attn_output = torch.matmul(attn_weights, V)
# attn_output: [batch_size, num_heads, seq_len, head_dim]

# 6. 应用门控机制 - element-wise加权
# gate_scores需要reshape以匹配attn_output的维度
gate_scores = gate_scores.transpose(1, 2).unsqueeze(-1)
# gate_scores: [batch_size, num_heads, seq_len, 1]

# Element-wise multiplication
gated_output = attn_output * gate_scores

```


- FFN


- KV-cache
	- [MTLA](https://arxiv.org/pdf/2505.13544) Multi-head temporal latent attention 在时间维度上压缩KV-cache，这个工作基于Deepseek的MLA
	- 这种方案不是Training-Free的，引入了投影层的来做降采样
	- DeepSeek的MLA也是一个用于优化KV-cache存储的工作，用low-rank分解的方案，存储KV矩阵的低秩分解，可以达到4-6倍的压缩率

但是这个做法并不够Make Sense，尤其是在NLP任务上，引入了过强的先验，因为并不是相邻的两个token，就是有足够的相关性，足以让它们压缩到一个更小的空间，这个做法往往会带来比较大的性能损失


```python

# KV cache 更新
def update(self, kv_norm_t, k_pe, layer_idx, abs_length):
    T = infer_steps  # 当前总token数
    T_remain = T % self.down_rate  # 当前在块内的位置
    
    if T_remain != 1:
        # 在同一个块内：累加更新
        prev_kv_t[:, -1:] += kv_norm_t
        prev_k_pe[:, -1:] = k_pe
    else:
        # 新块开始：追加新的cache entry
        prev_kv_t = torch.cat([prev_kv_t, kv_norm_t], dim=1)
        prev_k_pe = torch.cat([prev_k_pe, k_pe], dim=1)


# 迭代生成
if seqlen > 1:
    if self.recompute_prompt_attn:
        # 完整计算prompt的注意力
        w_tT = self.hypernet_down(T, t, train=True, ...)
        kv_norm_t = matmul(w_tT * zero_mask, kv_norm)
        # 但只缓存降采样后的indices
        past_key_value.update(prev_kv_t[:, indices], ...)
    else:
        # 直接只缓存降采样位置
        kv_norm_t = matmul(w_tT * zero_mask[indices], kv_norm)
```

![](asset/Pasted%20image%2020251216171650.png)

- [Nemotron](https://research.nvidia.com/labs/nemotron/files/NVIDIA-Nemotron-3-White-Paper.pdf)，nvidia开源的大模型，相比于市面上的开源大模型做了较大的改动
	- 架构：将Moe层和Mamba-2混合，降低成本，无需维护KV-cache（因为大量的层都使用了mamba，只有少部分层使用Attention的需要维护KV-cache）
	- 架构：latent-moe，在进行专家路由之前，先进行降维，再计算完成之后进行升维
	- 推理：MTP multi-token prediction
	- 训练：NVFP4训练
	- Long Context，使用mamba层来避免Rope的扩展退化，mamba作为维护一个状态的递归神经网络，其天然有位置编码的能力（因为早进入网络的token衰减得更多）

现阶段Mamba相比于一些SLA的项目，有更加成熟的算子库，因此很多系统都适用Mamba+Attention的操作

![](asset/Pasted%20image%2020251216194253.png)

![](asset/Pasted%20image%2020251216191413.png)

![](asset/Pasted%20image%2020251216192043.png)

## 12.20

- https://arxiv.org/abs/2512.13507
	- Seedance 1.5 pro：先进的音视频同出模型，可以借这个模型感受一下音视频同出模型现在的是怎么做的
- https://arxiv.org/pdf/2512.15603
	- Qwen-image-layered: 个人认为是一个非常用应用场景的模型（这种功能应该在传统cv里就有人做了），该模型已经开源权重，其作用是将生成好的图像进行自动分层
	- 区别在于传统的做法精度比较低，并且无法通过语义描述进行分割；但是相比于以前的模型，Qwen-image-layered是一个很大的模型，其资源消耗也大得多

## 12.22

- https://arxiv.org/pdf/2512.13687
	- MiniMax 提出的一种新型的vision tokenizer，对比VAE，其能够更好的scale，针对VAE的奇怪现象（指一是数据的scale无法促进重建效果的提高，二是重建效果的提高也无法使得生成的效果变好）
		- VAE的特性：只关注重建，过度关注底层纹理 -> latent space 一定要关注high-level语义
		- VTP：同时优化文本-图像对齐，自监督，重建三种损失，构建更加优质的latent space
	- VTP的训练
		- 模型架构：用的是vit
		- 训练目标：Vision reconstruction(VAE) + Self supervised learning(like DINO v2) + Contrastive learning(Clip) 

![](asset/Pasted%20image%2020251222205545.png)

![](asset/Pasted%20image%2020251222215147.png)

- https://github.com/thu-ml/TurboDiffusion
	- TurboDiffusion，一篇efficient diffusion的工作，组合了一个优化方案，使得video generation推理速度加快了上百倍，例如在5090上执行wan 2.1 1.3B 480P 可以达到1.9s的生成速度
	- 组合了SageAttention + SLA + rCM 蒸馏 + W8A8 量化
- https://arxiv.org/pdf/2512.17901
	- Law of Reasoning 从推理本身入手分析reasoning model，是否对于复杂问题，消耗的token线形增长，是否对于组合的问题，消耗的token和分别解决这两个问题相似，论文发现组合的问题会导致reasoning消耗的token过度增长，针对这种现象，合成了一波数据解决
		- 但是个人认为这篇论文的思考并不是很make sense，一来本身这些问题很少出现在语境中，二来这种解决方案也不是很优雅，个人感觉还是得依赖RL的算法来解决这类问题，这种造特定数据解决特定现象感觉是在打补丁


## 12.28

- https://dreamontage.github.io/DreaMontage/
	- dreamontage 字节新出的视频生成模型，支持内部任意帧的控制，更加灵活的视频生成


## 12.29

- https://zhuanlan.zhihu.com/p/1988221694591128167
	- 这是一篇有关于LLM推理架构演化的博客
	- 从Static batching -> Continuous batching (Paged Attention) -> Chunked Prefill -> PD分离（Prefill和Decoder分离） -> AF分离（Attention和FFN分离）这里的分离都是使用硬件的分离，发挥不同特性硬件的专长并且优化通信

